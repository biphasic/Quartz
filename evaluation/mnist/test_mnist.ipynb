{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import sinabs\n",
    "from torchvision import transforms, datasets\n",
    "import sinabs.layers as sl\n",
    "import numpy as np\n",
    "import quartz\n",
    "from quartz.utils import get_accuracy, encode_inputs, decode_outputs, normalize_outputs, plot_output_comparison, normalize_weights, plot_output_comparison_ann_snn, count_n_neurons, n_operations, omega_read, omega_write\n",
    "from mnist_model import ConvNet\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 200\n",
    "num_workers = 4\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "valid_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "norm_loader = DataLoader(dataset=valid_dataset, batch_size=10000, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"mnist-convnet.pth\", map_location=torch.device(device))\n",
    "model = ConvNet()\n",
    "\n",
    "model[0].weight.data = state_dict['conv1.weight']\n",
    "model[0].bias.data = state_dict['conv1.bias']\n",
    "model[3].weight.data = state_dict['conv2.weight']\n",
    "model[3].bias.data = state_dict['conv2.bias']\n",
    "model[6].weight.data = state_dict['conv3.weight']\n",
    "model[6].bias.data = state_dict['conv3.bias']\n",
    "model[10].weight.data = state_dict['fc1.weight']\n",
    "model[10].bias.data = state_dict['fc1.bias']\n",
    "\n",
    "model.eval();\n",
    "# get_accuracy(model, valid_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)/1_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_synops = 31570 # calculated further down, search for n_ops\n",
    "n_neurons = count_n_neurons(model.cpu(), next(iter(valid_loader))[0][0:1], add_last_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
    "flops = FlopCountAnalysis(model, next(iter(valid_loader))[0])\n",
    "print(flops.total()/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "for percentile in [98, 99, 99.5, 99.9, 99.99, 99.999]:\n",
    "    norm_model = copy.deepcopy(model)\n",
    "    normalize_outputs(norm_model.to(device), sample_data=next(iter(norm_loader))[0].to(device), percentile=percentile, max_outputs=[])\n",
    "\n",
    "    for exponent in range(1, 6):\n",
    "        t_max = 2**exponent\n",
    "        snn = quartz.from_torch.from_model(norm_model, t_max=t_max, add_spiking_output=True).to(device).eval()\n",
    "        metric = get_accuracy(snn, valid_loader, device, t_max=t_max, calculate_early_spikes=True, calculate_output_time=True)\n",
    "        metric[t_max]['n_ops'] = n_operations(n_neurons, t_max, n_synops)\n",
    "        metric[t_max]['read_ops'] = omega_read(n_neurons, t_max, n_synops)\n",
    "        metric[t_max]['write_ops'] = omega_write(n_neurons, t_max, n_synops)\n",
    "        if percentile in stats.keys():\n",
    "            stats[percentile].update(metric)\n",
    "        else:\n",
    "            stats[percentile] = metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mnist-results.pkl', 'wb') as file:\n",
    "    pickle.dump(stats, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_model = deepcopy(model)\n",
    "normalize_outputs(norm_model.to(device), all_test_images.to(device), percentile=percentile, max_outputs=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu = 'cpu'\n",
    "# t_max = 2**6\n",
    "\n",
    "# sample_data = next(iter(valid_loader))[0]\n",
    "\n",
    "# snn = quartz.from_torch.from_model(norm_model, t_max=t_max, add_spiking_output=True).to(cpu).eval()\n",
    "# snn_output_layers = [name for name, child in snn.named_children() if isinstance(child, quartz.IF)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_output_comparison_model_snn(norm_model.to(cpu), snn.to(cpu), sample_data.to(cpu), \n",
    "#     ann_output_layers=output_layer_names, \n",
    "#     snn_output_layers=snn_output_layers, \n",
    "#     t_max=t_max, \n",
    "#     every_n=10, \n",
    "#     every_c=1, \n",
    "#     savefig=f\"ann-snn-comparison-tmax{t_max}-percentile{percentile}.png\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_model = deepcopy(norm_model)\n",
    "# for exponent in range(1, 7):\n",
    "#     t_max = 2**exponent\n",
    "#     def quantize(module, input, output):\n",
    "#         return (output * t_max).round() / t_max\n",
    "\n",
    "#     for module in q_model.children():\n",
    "#         if isinstance(module, nn.ReLU):\n",
    "#             module.register_forward_hook(quantize)\n",
    "#     q_model[-1].register_forward_hook(quantize)\n",
    "\n",
    "#     accuracy = get_accuracy(q_model, valid_loader, device)\n",
    "#     print(f\"{t_max} time steps: {round(accuracy, 3)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([3.30459, 2.20934]).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for exponent in range(1, 6):\n",
    "    t_max = 2**exponent\n",
    "    snn = quartz.from_torch.from_model(norm_model, t_max=t_max, add_spiking_output=True).to(device).eval()\n",
    "    metric = get_accuracy(snn, valid_loader, device, t_max=t_max, calculate_early_spikes=True, calculate_output_time=True)\n",
    "    metrics.append(metric)\n",
    "    print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for percentile in [97, 98, 99, 99.9, 99.99, 99.999]: # [99]: #\n",
    "    norm_model = deepcopy(model)\n",
    "    normalize_outputs(norm_model.to(device), all_test_images.to(device), percentile=percentile, max_outputs=[])\n",
    "    t_max = 16\n",
    "    snn = quartz.from_torch.from_model(norm_model, t_max=t_max, add_spiking_output=True).to(device).eval()\n",
    "    accuracy = get_accuracy(snn, valid_loader, device, t_max=t_max, calculate_early_spikes=True, calculate_output_time=True)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 - np.array(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ops = [layer.n_ops for layer in snn if isinstance(layer, sl.StatefulLayer)]\n",
    "torch.stack(n_ops).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.n_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max = 2**3+1\n",
    "images, label = next(iter(valid_loader))\n",
    "spikes = encode_inputs(images, t_max=t_max).to(device)\n",
    "snn = quartz.from_torch.from_model(norm_model, t_max=t_max, add_spiking_output=True).to(device).eval()\n",
    "output_layers = [child for name, child in snn.named_children() if isinstance(child, sl.StatefulLayer)]\n",
    "\n",
    "plot_output_histograms(snn, spikes, output_layers, t_max=t_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max = 2**3+1\n",
    "def quantize(module, input, output):\n",
    "    return (output * t_max).int() / t_max\n",
    "\n",
    "for module in model.children():\n",
    "    if isinstance(module, nn.ReLU):\n",
    "        module.register_forward_hook(quantize)\n",
    "\n",
    "param_layers = [child for name, child in model.named_children() if isinstance(child, (nn.Conv2d, nn.Linear))]\n",
    "output_layers = [child for name, child in model.named_children() if isinstance(child, nn.ReLU)]\n",
    "output_layers += [param_layers[-1]]\n",
    "plot_output_histograms(model, images.to(device), output_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max = 2**5\n",
    "snn = quartz.from_torch.from_model(model, t_max=t_max, add_spiking_output=True).to(device).eval()\n",
    "synop_counter = sinabs.SNNSynOpCounter(snn)\n",
    "accuracy = get_accuracy(snn, valid_loader, device, t_max=t_max)\n",
    "print(f\"{t_max} time steps: {round(accuracy, 3)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synop_counter.get_total_synops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synop_counter.get_total_synops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.4933e+08"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c00e5e7c7a569083cb991dfa106f557879cc0d1d84bf5b9d92fbb6bf680d358"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
